# COPP Visualization and Multi-Mode Sonification Fix
# Add this code to your notebook after data analysis is complete

import matplotlib.pyplot as plt
import numpy as np
from scipy.io import wavfile
import base64
from io import BytesIO
from IPython.display import HTML, display, Audio

# Assume 'results' is your analysis results list
# Convert to DataFrame for easier plotting
import pandas as pd
df = pd.DataFrame(results)

print(f"Loaded {len(df)} data points from {df['time_s'].min():.1f}s to {df['time_s'].max():.1f}s")

# ============================================================================
# VISUALIZATION SECTION - Fixed for HTML display
# ============================================================================

def create_plot_html(fig):
    """Convert matplotlib figure to base64 HTML image"""
    buf = BytesIO()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight', facecolor='#0f0f1e')
    buf.seek(0)
    img_base64 = base64.b64encode(buf.read()).decode('utf-8')
    plt.close()
    return f'<img src="data:image/png;base64,{img_base64}" style="max-width:100%; border-radius:8px;">'

# PLOT 1: Intensity Time Series
print("Creating Intensity Time Series plot...")
fig, ax = plt.subplots(figsize=(14, 5), facecolor='#0f0f1e')
ax.plot(df['time_s'], df['peak_intensity'], color='#00d4ff', linewidth=2, label='Peak Intensity')
ax.fill_between(df['time_s'], df['peak_intensity'], alpha=0.3, color='#00d4ff')
ax.set_xlabel('Time (seconds)', color='white', fontsize=12)
ax.set_ylabel('Intensity', color='white', fontsize=12)
ax.set_title('Intensity Time Series', color='#00d4ff', fontsize=16, fontweight='bold', pad=20)
ax.grid(True, alpha=0.2, color='white')
ax.set_facecolor('#1a1a2e')
ax.tick_params(colors='white')
for spine in ax.spines.values():
    spine.set_edgecolor('#00d4ff')
    spine.set_linewidth(2)
display(HTML(create_plot_html(fig)))

# PLOT 2: Spectral Channel Decomposition (RGB)
print("Creating RGB Spectral Decomposition plot...")
fig, ax = plt.subplots(figsize=(14, 5), facecolor='#0f0f1e')
ax.plot(df['time_s'], df['r_channel'], color='#ff4444', linewidth=2, label='Red', alpha=0.8)
ax.plot(df['time_s'], df['g_channel'], color='#44ff44', linewidth=2, label='Green', alpha=0.8)
ax.plot(df['time_s'], df['b_channel'], color='#4444ff', linewidth=2, label='Blue', alpha=0.8)
ax.set_xlabel('Time (seconds)', color='white', fontsize=12)
ax.set_ylabel('Channel Intensity', color='white', fontsize=12)
ax.set_title('Spectral Channel Decomposition (RGB)', color='#00d4ff', fontsize=16, fontweight='bold', pad=20)
ax.legend(loc='upper right', facecolor='#1a1a2e', edgecolor='#00d4ff', labelcolor='white')
ax.grid(True, alpha=0.2, color='white')
ax.set_facecolor('#1a1a2e')
ax.tick_params(colors='white')
for spine in ax.spines.values():
    spine.set_edgecolor('#00d4ff')
    spine.set_linewidth(2)
display(HTML(create_plot_html(fig)))

# PLOT 3: Apparent Size Over Time
print("Creating Apparent Size plot...")
fig, ax = plt.subplots(figsize=(14, 5), facecolor='#0f0f1e')
ax.plot(df['time_s'], df['size_px'], color='#ffaa00', linewidth=2, marker='o', markersize=3)
ax.set_xlabel('Time (seconds)', color='white', fontsize=12)
ax.set_ylabel('Apparent Size (pixels)', color='white', fontsize=12)
ax.set_title('Apparent Size Over Time', color='#00d4ff', fontsize=16, fontweight='bold', pad=20)
ax.grid(True, alpha=0.2, color='white')
ax.set_facecolor('#1a1a2e')
ax.tick_params(colors='white')
for spine in ax.spines.values():
    spine.set_edgecolor('#00d4ff')
    spine.set_linewidth(2)
display(HTML(create_plot_html(fig)))

# PLOT 4: 2D Trajectory
print("Creating 2D Trajectory plot...")
fig, ax = plt.subplots(figsize=(10, 8), facecolor='#0f0f1e')
scatter = ax.scatter(df['cx'], df['cy'], c=df['time_s'], cmap='plasma', s=50, alpha=0.6)
ax.plot(df['cx'], df['cy'], color='white', alpha=0.3, linewidth=1)
# Mark start and end
ax.scatter(df['cx'].iloc[0], df['cy'].iloc[0], color='#00ff00', s=200, marker='o', 
           edgecolors='white', linewidths=2, label='Start', zorder=5)
ax.scatter(df['cx'].iloc[-1], df['cy'].iloc[-1], color='#ff0000', s=200, marker='X', 
           edgecolors='white', linewidths=2, label='End', zorder=5)
ax.set_xlabel('X Position (pixels)', color='white', fontsize=12)
ax.set_ylabel('Y Position (pixels)', color='white', fontsize=12)
ax.set_title('Orb 2D Trajectory', color='#00d4ff', fontsize=16, fontweight='bold', pad=20)
ax.invert_yaxis()  # Invert Y axis to match image coordinates
ax.legend(loc='upper right', facecolor='#1a1a2e', edgecolor='#00d4ff', labelcolor='white')
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('Time (s)', color='white', fontsize=10)
cbar.ax.tick_params(colors='white')
ax.set_facecolor('#1a1a2e')
ax.tick_params(colors='white')
for spine in ax.spines.values():
    spine.set_edgecolor('#00d4ff')
    spine.set_linewidth(2)
display(HTML(create_plot_html(fig)))

print("âœ“ All visualizations created successfully!")

# ============================================================================
# MULTI-MODE SONIFICATION SECTION
# ============================================================================

print("\n" + "="*60)
print("MULTI-MODE SONIFICATION")
print("="*60)

SAMPLE_RATE = 48000
MIN_FREQ = 200  # Hz
MAX_FREQ = 2000  # Hz

def normalize_to_range(data, min_val, max_val):
    """Normalize data to specified range"""
    data_min, data_max = np.min(data), np.max(data)
    if data_max == data_min:
        return np.full_like(data, (min_val + max_val) / 2)
    normalized = (data - data_min) / (data_max - data_min)
    return min_val + normalized * (max_val - min_val)

def generate_tone_sequence(frequencies, duration_per_point=0.15):
    """Generate smooth tone sequence from frequency array"""
    samples_per_point = int(SAMPLE_RATE * duration_per_point)
    audio = []
    
    for i, freq in enumerate(frequencies):
        t = np.linspace(0, duration_per_point, samples_per_point, False)
        tone = np.sin(2 * np.pi * freq * t)
        
        # Apply envelope for smooth transitions
        envelope = np.ones_like(tone)
        fade_samples = int(samples_per_point * 0.1)
        envelope[:fade_samples] = np.linspace(0, 1, fade_samples)
        envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)
        
        audio.extend(tone * envelope)
    
    return np.array(audio)

# MODE 1: PULSATION ONLY (Intrinsic Voice)
print("\n[1/3] Generating PULSATION-ONLY sonification...")
print("      Mapping: Brightness â†’ Pitch (intrinsic orb energy)")

pulsation_freqs = normalize_to_range(df['peak_intensity'].values, MIN_FREQ, MAX_FREQ)
pulsation_audio = generate_tone_sequence(pulsation_freqs)
pulsation_audio = (pulsation_audio * 32767 * 0.8).astype(np.int16)

pulsation_file = 'orb_pulsation_only.wav'
wavfile.write(pulsation_file, SAMPLE_RATE, pulsation_audio)
print(f"      âœ“ Saved: {pulsation_file}")
print(f"      Duration: {len(pulsation_audio)/SAMPLE_RATE:.1f}s")
print(f"      Frequency range: {MIN_FREQ}-{MAX_FREQ} Hz")

# MODE 2: TRAJECTORY ONLY (Movement Voice)
print("\n[2/3] Generating TRAJECTORY-ONLY sonification...")
print("      Mapping: X-position â†’ Pitch, Y-position â†’ Volume")

# X position controls pitch
trajectory_freqs = normalize_to_range(df['cx'].values, MIN_FREQ, MAX_FREQ)
# Y position controls amplitude (volume)
trajectory_amps = normalize_to_range(df['cy'].values, 0.3, 1.0)

trajectory_audio = []
duration_per_point = 0.15
samples_per_point = int(SAMPLE_RATE * duration_per_point)

for freq, amp in zip(trajectory_freqs, trajectory_amps):
    t = np.linspace(0, duration_per_point, samples_per_point, False)
    tone = np.sin(2 * np.pi * freq * t) * amp
    
    # Envelope
    envelope = np.ones_like(tone)
    fade_samples = int(samples_per_point * 0.1)
    envelope[:fade_samples] = np.linspace(0, 1, fade_samples)
    envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)
    
    trajectory_audio.extend(tone * envelope)

trajectory_audio = np.array(trajectory_audio)
trajectory_audio = (trajectory_audio * 32767 * 0.8).astype(np.int16)

trajectory_file = 'orb_trajectory_only.wav'
wavfile.write(trajectory_file, SAMPLE_RATE, trajectory_audio)
print(f"      âœ“ Saved: {trajectory_file}")
print(f"      Duration: {len(trajectory_audio)/SAMPLE_RATE:.1f}s")

# MODE 3: COMBINED (Original - All Data)
print("\n[3/3] Generating COMBINED sonification...")
print("      Mapping: Brightness â†’ Pitch, Size â†’ Volume, Blue-excess â†’ Harmonics")

combined_freqs = normalize_to_range(df['peak_intensity'].values, MIN_FREQ, MAX_FREQ)
combined_amps = normalize_to_range(df['size_px'].values, 0.4, 1.0)

# Add harmonic richness based on blue excess
blue_excess_normalized = normalize_to_range(df['normalized_blue_excess'].fillna(0).values, 0, 0.5)

combined_audio = []
for freq, amp, blue in zip(combined_freqs, combined_amps, blue_excess_normalized):
    t = np.linspace(0, duration_per_point, samples_per_point, False)
    
    # Fundamental + harmonics (richer for blue excess)
    fundamental = np.sin(2 * np.pi * freq * t)
    harmonic2 = np.sin(2 * np.pi * freq * 2 * t) * (0.3 + blue)
    harmonic3 = np.sin(2 * np.pi * freq * 3 * t) * (0.15 + blue * 0.5)
    
    tone = (fundamental + harmonic2 + harmonic3) / (1.45 + blue * 1.5) * amp
    
    # Envelope
    envelope = np.ones_like(tone)
    fade_samples = int(samples_per_point * 0.1)
    envelope[:fade_samples] = np.linspace(0, 1, fade_samples)
    envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)
    
    combined_audio.extend(tone * envelope)

combined_audio = np.array(combined_audio)
combined_audio = (combined_audio * 32767 * 0.8).astype(np.int16)

combined_file = 'orb_combined_full.wav'
wavfile.write(combined_file, SAMPLE_RATE, combined_audio)
print(f"      âœ“ Saved: {combined_file}")
print(f"      Duration: {len(combined_audio)/SAMPLE_RATE:.1f}s")

# ============================================================================
# DISPLAY AUDIO PLAYERS
# ============================================================================

print("\n" + "="*60)
print("SONIFICATION COMPLETE")
print("="*60)

display(HTML("""
<div style="background: #1a1a2e; padding: 30px; border-radius: 12px; border: 2px solid #00d4ff; margin-top: 20px;">
    <h2 style="color: #00d4ff; font-family: 'Courier New', monospace; margin-bottom: 20px;">
        ðŸŽµ Multi-Mode Orb Sonification
    </h2>
    <p style="color: white; font-size: 14px; line-height: 1.6; margin-bottom: 25px;">
        The orb's photometric data mapped to sound: brightness controls pitch (pentatonic scale), 
        apparent size controls volume, and blue excess controls harmonic richness.
    </p>
</div>
"""))

# Display each audio mode
print("\nðŸŽµ MODE 1: PULSATION ONLY (Intrinsic Voice)")
print("   Listen to the orb's brightness changes without trajectory noise")
display(Audio(pulsation_file))

print("\nðŸŽµ MODE 2: TRAJECTORY ONLY (Movement Voice)")  
print("   Listen to the orb's spatial movement: Xâ†’Pitch, Yâ†’Volume")
display(Audio(trajectory_file))

print("\nðŸŽµ MODE 3: COMBINED (Full Signal)")
print("   All data combined: Brightnessâ†’Pitch, Sizeâ†’Volume, Blue-excessâ†’Harmonics")
display(Audio(combined_file))

print("\n" + "="*60)
print("âœ“ Analysis Complete - Three Voices of the Orb Revealed")
print("="*60)
